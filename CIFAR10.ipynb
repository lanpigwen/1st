{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0qrmfsNqtRgDylGgsyVrM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanpigwen/1st/blob/main/CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ldfkhWLQbZ",
        "outputId": "0e820a1f-d849-4924-d3c5-bfdeccad4579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch[1/10] train: 100%|██████████| 391/391 [00:11<00:00, 32.88it/s, acc=0.119, loss=2.27]\n",
            "Epoch[1/10] test: 100%|██████████| 79/79 [00:01<00:00, 41.27it/s, acc=0.17, loss=2.16]\n",
            "Epoch[2/10] train: 100%|██████████| 391/391 [00:13<00:00, 29.54it/s, acc=0.243, loss=2.09]\n",
            "Epoch[2/10] test: 100%|██████████| 79/79 [00:01<00:00, 41.87it/s, acc=0.297, loss=1.72]\n",
            "Epoch[3/10] train: 100%|██████████| 391/391 [00:11<00:00, 32.93it/s, acc=0.317, loss=1.98]\n",
            "Epoch[3/10] test: 100%|██████████| 79/79 [00:01<00:00, 40.73it/s, acc=0.355, loss=1.71]\n",
            "Epoch[4/10] train: 100%|██████████| 391/391 [00:11<00:00, 32.97it/s, acc=0.366, loss=1.89]\n",
            "Epoch[4/10] test: 100%|██████████| 79/79 [00:02<00:00, 34.87it/s, acc=0.38, loss=1.71]\n",
            "Epoch[5/10] train: 100%|██████████| 391/391 [00:11<00:00, 33.14it/s, acc=0.402, loss=1.81]\n",
            "Epoch[5/10] test: 100%|██████████| 79/79 [00:02<00:00, 35.31it/s, acc=0.41, loss=1.7]\n",
            "Epoch[6/10] train: 100%|██████████| 391/391 [00:11<00:00, 32.86it/s, acc=0.429, loss=1.76]\n",
            "Epoch[6/10] test: 100%|██████████| 79/79 [00:01<00:00, 41.90it/s, acc=0.435, loss=1.65]\n",
            "Epoch[7/10] train: 100%|██████████| 391/391 [00:11<00:00, 33.26it/s, acc=0.449, loss=1.71]\n",
            "Epoch[7/10] test: 100%|██████████| 79/79 [00:01<00:00, 41.47it/s, acc=0.454, loss=1.61]\n",
            "Epoch[8/10] train: 100%|██████████| 391/391 [00:11<00:00, 33.04it/s, acc=0.466, loss=1.67]\n",
            "Epoch[8/10] test: 100%|██████████| 79/79 [00:01<00:00, 42.06it/s, acc=0.468, loss=1.58]\n",
            "Epoch[9/10] train: 100%|██████████| 391/391 [00:11<00:00, 32.72it/s, acc=0.483, loss=1.64]\n",
            "Epoch[9/10] test: 100%|██████████| 79/79 [00:01<00:00, 42.40it/s, acc=0.487, loss=1.51]\n",
            "Epoch[10/10] train: 100%|██████████| 391/391 [00:11<00:00, 32.97it/s, acc=0.496, loss=1.61]\n",
            "Epoch[10/10] test: 100%|██████████| 79/79 [00:01<00:00, 42.31it/s, acc=0.501, loss=1.44]\n"
          ]
        }
      ],
      "source": [
        "import torch.nn\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear\n",
        "\n",
        "class CifarNN10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.module1 = Sequential(\n",
        "            Conv2d(3, 32, 5, padding=2),\n",
        "            MaxPool2d(2),\n",
        "            Conv2d(32, 32, 5, padding=2),\n",
        "            MaxPool2d(2),\n",
        "            Conv2d(32, 64, 5, padding=2),\n",
        "            MaxPool2d(2),\n",
        "            Flatten(),\n",
        "            Linear(1024, 64),\n",
        "            Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.module1(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "# 准备数据\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"datasets\", train=True, transform=torchvision.transforms.ToTensor(),\n",
        "                                             download=True)\n",
        "dataset_test = torchvision.datasets.CIFAR10(\"datasets\", train=False, transform=torchvision.transforms.ToTensor(),\n",
        "                                            download=True)\n",
        "# dataloader_train = DataLoader(dataset_train, batch_size=128, drop_last=True)\n",
        "# dataloader_test = DataLoader(dataset_test, batch_size=128, drop_last=True)\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=128, drop_last=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=128, drop_last=False)\n",
        "\n",
        "train_size = len(dataset_train)\n",
        "test_size = len(dataset_test)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# 构建模型 从p29model中import\n",
        "nnModel = CifarNN10()\n",
        "nnModel.to(device)\n",
        "\n",
        "# 定义损失函数\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss_fn.to(device)\n",
        "\n",
        "# 定义优化器\n",
        "learn_rate = 1e-2\n",
        "optimizer = torch.optim.SGD(nnModel.parameters(), lr=learn_rate)\n",
        "\n",
        "# tensorboard\n",
        "writer = SummaryWriter(\"p29Log\")\n",
        "\n",
        "total_train_step = 0\n",
        "total_test_step = 0\n",
        "num_epoch = 10\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    # print(f'第{epoch+1}轮训练开始')\n",
        "    # total_train_loss=0\n",
        "    total_train_data = 0\n",
        "    total_train_accuracy = 0\n",
        "\n",
        "    nnModel.train()\n",
        "    # nnModel.train()/.eval()只会影响dropout 还有一个什么\n",
        "\n",
        "    train_loop = tqdm((dataloader_train), total=len(dataloader_train))\n",
        "    for data in train_loop:\n",
        "        imgs, targets = data\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        outputs = nnModel(imgs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        # 优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # tensorboard显示\n",
        "        total_train_step += 1\n",
        "        writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
        "\n",
        "        accuracy = (outputs.argmax(1) == targets).sum()\n",
        "        total_train_accuracy += accuracy\n",
        "        total_train_data += len(imgs)\n",
        "        accuracy = total_train_accuracy / float(total_train_data)\n",
        "\n",
        "        train_loop.set_description(f'Epoch[{epoch + 1}/{num_epoch}] train')\n",
        "        train_loop.set_postfix(loss=loss.item(), acc=accuracy.item())\n",
        "\n",
        "        # if total_train_step % 100 == 0:\n",
        "        #     print(f'----第{total_train_step}次数据训练---- loss:{loss.item()}')\n",
        "\n",
        "    # 测试时，不要影响grad\n",
        "    nnModel.eval()\n",
        "    total_test_loss = 0\n",
        "    total_test_accuracy = 0\n",
        "    total_test_data = 0\n",
        "    with torch.no_grad():\n",
        "        test_loop = tqdm((dataloader_test), total=len(dataloader_test))\n",
        "        for data in test_loop:\n",
        "            imgs, targets = data\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            outputs = nnModel(imgs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            accuracy = (outputs.argmax(1) == targets).sum()\n",
        "            total_test_accuracy += accuracy\n",
        "            total_test_data += len(imgs)\n",
        "            accuracy = total_test_accuracy / float(total_test_data)\n",
        "\n",
        "            test_loop.set_description(f'Epoch[{epoch + 1}/{num_epoch}] test')\n",
        "            test_loop.set_postfix(loss=loss.item(), acc=accuracy.item())\n",
        "\n",
        "    total_test_accuracy = total_test_accuracy / float(test_size)\n",
        "    # test_loop.write(f'第{epoch + 1}轮训练:  loss:{total_test_loss}   accuracy:{total_test_accuracy}')\n",
        "    # print(f'第{epoch+1}轮训练:  loss:{total_test_loss}   accuracy:{total_test_accuracy}')\n",
        "    # tensorboard\n",
        "    total_test_step += 1\n",
        "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
        "    writer.add_scalar(\"test_accuracy\", total_test_accuracy, total_test_step)\n",
        "\n",
        "writer.close()\n"
      ]
    }
  ]
}